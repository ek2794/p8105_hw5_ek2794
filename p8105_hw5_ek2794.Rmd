---
title: "Homework 5"
output: github_document
---

Let's set up our homework!
```{r}
library(tidyverse)
library(dplyr)
library(readr)
library(readxl)
library(stringr)
library(broom)
library(patchwork)
set.seed(1)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


```
## Here I downloaded the dataset from Github. I created a separated city_state variable to identify each individual city, which led us to 51 unique cities. The homicides are divided into Closed by arrest which are the closed cases and the unsolved_sum which are the sum of "Closed without arrest" or "Open/No arrest". n_obs represents the total number of homicides observed in the city_state.

To test out the prop.test function, I created a separate baltimore function by filtering out just Baltimore from the crimes_unsolved dataset.##


```{r}
website = ("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")

crimes_df = read_csv(website) %>%
  janitor::clean_names() %>%
  mutate(
  city_state = paste(city, state, sep = "_")
)

crimes_totalcrimes = crimes_df %>%
  group_by(city_state) %>%
  summarize(n_obs = n())
  
crimes_unsolved_categories = crimes_df %>%
  group_by(disposition, city_state) %>% 
  summarize(n_obs = n()) %>%
  pivot_wider(
    names_from = "disposition",
    values_from = "n_obs") %>%
  select("city_state", "Closed without arrest", "Open/No arrest")
  
unsolved_sum = 
  rowSums(crimes_unsolved_categories[,c("Closed without arrest", "Open/No arrest")], na.rm = TRUE) 

crimes_unsolved_total = cbind(crimes_totalcrimes, unsolved_sum) 
crimes_unsolved = crimes_unsolved_total[, c(1, 3, 2)]
  
baltimore = crimes_unsolved %>%
  filter(city_state == "Baltimore_MD")

```


##Here I created a function for proptest where x = the # of unsolved crimes and n = total number of homicides. I used the 'broom::tidy' function to create a workable tibble and selected the estimate, low, and high confidence interval. I created the proptest into a function in order to be able to be applied to the rest of the cities.##

```{r}

proptest = function(unsolved_sum, n_obs) {

    prop.test(unsolved_sum, n_obs, p = 0.5, alternative = "two.sided") %>%
    broom::tidy() %>%
    select(estimate, conf.low, conf.high)
}

```


##I successfully ran the proptest function with the baltimore dataset. Subsequently, I used the map function to apply the 'prop.test' function to the larger cities_unsolved dataset, using the broom::tidy to create a tidy dataframe. Subsequently, I unnested the dataset and then selected the same 3 variables again to generate a dataframe of estimated proportions and CIs for each city. ##


```{r}

baltimore_results = 
  proptest(
    pull(baltimore, unsolved_sum), 
    pull(baltimore, n_obs)) 


cities_results =
  crimes_unsolved %>%
  mutate(
  prop_data = map2(unsolved_sum, n_obs, ~prop.test(.x, .y) %>%
    broom::tidy())
) %>%
  unnest(prop_data) %>%
  select(city_state, estimate, conf.low, conf.high) %>%
  rename(
    conf_low = conf.low,
    conf_high = conf.high)
  
cities_results

```


```{r}
cities_results %>%
  mutate(
  city_state = fct_reorder(city_state, estimate, .desc = TRUE)) %>%
  ggplot(
    aes(x = city_state, y = estimate)) + 
    geom_point() + 
    geom_errorbar(aes(ymin = conf_low, ymax = conf_high)) +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

##Using the results from the tidy dataframe above, I plotted the each city in descending order of estimates using ggplot. The confidence intervals are demonstrated with ymin as conf_low and ymax as conf_high illustrated with error bars. The fct_reorder function was used to organize the cities in this descending order.##



##PROBLEM 3 

When designing an experiment or analysis, a common question is whether it is likely that a true effect will be detected – put differently, whether a false null hypothesis will be rejected. The probability that a false null hypothesis is rejected is referred to as power, and it depends on several factors, including: the sample size; the effect size; and the error variance. In this problem, you will conduct a simulation to explore power in a one-sample t-test.

First set the following design elements:

Fix n=30
Fix σ=5
Set μ=0. Generate 5000 datasets from the model


##Here we create a function within a function. The function on the top is the t-test function that we will feed in the dataframe into. Below we create a generate results function that applies the t test to a generated dataset with a for loop. 

Below the t test function we generate the dataset by establishing the mu of 0, standard deviation of 5, iterations of 5000, and observations of 30 per dataset. We create a for loop to be able to run this 5000 times and generate a new list with this. 

Next, we create a tibble which pulls on the prior data and applies the t-test function. This data is unnested. Subsequently the estimate/proportions and p-values are pulled out. This tibble is piped into the final tibble which gives summary statistics that shows whether or not the null was rejected with a p-value of <0.05 demonstrated with the rej variable. From this the proportion of times 'rej' was rejected is created and the average mu hat is created with the average of the proportions. 

Finally, we a tibble of mus 0 to 6 and run the map function to apply the entire generate_results function to so that a new tibble can be made with the summary statistics of all the 7 different mus. 

Additionally, a rejected only group was made by filtering out only the values for which the null hypothesis was rejected. 
```{r}

t_test_clean = function(df){
  out_df <- t.test(df,
         alternative = c("two.sided"),
         mu = 0, 
         conf.level = 0.95) %>%
    broom::tidy()
  return(out_df)
}



generate_results = function(mu = 0, 
                            sigma = 5, 
                            n_iter = 50, 
                            n_obs = 30){

new_list = list()



for (i in 1:n_iter) {
  temp_vec = rnorm(n = n_obs, mean = mu, sd = sigma)
  new_list[[i]] = temp_vec
}




stats_tib = 
  tibble(
    data = new_list
  ) %>%
  mutate(
    ttest_results = map(.x = data, ~t_test_clean(.x))
  ) %>%
  unnest(ttest_results) %>%
  janitor::clean_names() %>%
  select(estimate, p_value)

fin_tib = stats_tib %>%
  mutate(
    rej = p_value < 0.05
  ) %>%
  summarize(
    rej_value = rej,
    tot = n(),
    tot_rej = sum(rej),
    prop_rej = tot_rej/tot,
    avg_mu_hat = mean(estimate)
  ) %>%
return(fin_tib)
}



new_tib = 
  tibble(
    mu = c(0, 1, 2, 3, 4, 5, 6)
  ) %>%
  mutate(
    results = map(.x = mu, ~generate_results(mu = .x))) %>%
      unnest(results)

  

```




x∼Normal[μ,σ]

For each dataset, save μ̂  and the p-value arising from a test of H:μ=0 using α=0.05. Hint: to obtain the estimate and p-value, use broom::tidy to clean the output of t.test.

Repeat the above for μ={1,2,3,4,5,6}, and complete the following:



Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. Describe the association between effect size and power. 

Make a plot showing the average estimate of μ̂  on the y axis and the true value of μ on the x axis.

Make a second plot (or overlay on the first) the average estimate of μ̂  only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. Is the sample average of μ̂  across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?




```{r}

t_test_clean = function(df){
  out_df <- t.test(df,
         alternative = c("two.sided"),
         mu = 0, 
         conf.level = 0.95) %>%
    broom::tidy()
  return(out_df)
}



generate_results2 = function(mu = 0, 
                            sigma = 5, 
                            n_iter = 50, 
                            n_obs = 30){

new_list2 = list()



for (i in 1:n_iter) {
  temp_vec = rnorm(n = n_obs, mean = mu, sd = sigma)
  new_list2[[i]] = temp_vec
}




stats_tib2 = 
  tibble(
    data = new_list2
  ) %>%
  mutate(
    ttest_results = map(.x = data, ~t_test_clean(.x))
  ) %>%
  unnest(ttest_results) %>%
  janitor::clean_names() %>%
  select(estimate, p_value)

fin_tib2 = stats_tib2 %>%
  mutate(
    rej = p_value < 0.05
  ) %>%
  filter(p_value <0.05) %>%
  summarize(
    tot = n(),
    tot_rej = sum(rej),
    prop_rej = tot_rej/tot,
    avg_mu_hat = mean(estimate)
  ) %>%
return(fin_tib2)
}



new_tib2 = 
  tibble(
    mu = c(0, 1, 2, 3, 4, 5, 6)
  ) %>%
  mutate(
    results2 = map(.x = mu, ~generate_results2(mu = .x))) %>%
      unnest(results2)



rejectedonly = new_tib2 %>%
  ggplot(aes(x = mu, y = avg_mu_hat)) +
  geom_point(aes(color = mu)) 
  
  
```


```{r}


new_tib %>%
  ggplot(aes(x = mu, y = prop_rej)) + 
  geom_point(aes(color = mu))


all = new_tib %>%
  ggplot(aes(x = mu, y = avg_mu_hat)) +
  geom_point(aes(color = mu)) 
  
all + rejectedonly
```


